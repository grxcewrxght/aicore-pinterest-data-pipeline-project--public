# Pinterest Data Pipeline Project

## Contents 
1. **Project Brief**
2. **Project Dependencies**
3. **Project Walkthrough**
   - Batch Processing: *Configure the EC2 Kafka Client*
   - Batch Processing: *Connect a MSK Cluster to an S3 Bucket*
   - Batch Processing: *Configuring an API in API Gateway*
   - Batch Processing: *Databricks*
   - Batch Processing: *Spark on Databricks*
   - Batch Processing: *AWS MWAA*
   - Stream Processing: *AWS Kinesis*
4. **Looking Forward**



## 1. Project Brief

This project aims to build the system that Pinterest uses to analyse by historical and real-time data generated by posts from their users.

Pinterest boasts state-of-the-art machine learning engineering systems, handling billions of daily user interactions, including image uploads and clicks. The need to process this massive influx of data prompts the development of a cloud-based system in this project. The system features two distinct pipelines: one dedicated to real-time event processing for immediate metrics computation (e.g., profile popularity for instant recommendations), and another focused on historical data metrics computation (e.g., determining the most popular category for each year, past and present). This initiative aims to leverage advanced machine learning infrastructure to effectively inform decision-making at Pinterest.

## 2. Project Dependencies

To execute this project, the following modules must be installed: 
- `python-dotenv`
- `sqlalchemy`
- `requests`

To execute this, run the following commands on your CLI: 

```
pip install python-dotenv
```

```
pip install sqlalchemy
```

```
pip install requests
```


If you are using Anaconda and virtual environments (recommended), the Conda environment can be cloned by running the following
command, ensuring that env.yml is present in the project:

```
conda create env -f env.yml -n $ENVIRONMENT_NAME
```


**TOOLS AND DEPENDENCIES**

- **Apache Kafka :**  
 Apache kafka is an unified event streaming platform for handling all real-time data feeds (for example, Internet of Thing sensors and smartphones). It combines messaging, storage, and stream processing to allow storage and analysis of both historical and real-time data. The documention can be found [here](https://kafka.apache.org/documentation/). 

- **AWS MSK :**  
Amazon Managed Streaming for Apache Kafka (MSK) is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data. The guide can be found [here](https://docs.aws.amazon.com/msk/latest/developerguide/what-is-msk.html).

- **AWS MSK Connect :**  
MSK connect is a feature of Amazon MSK, that makes it easy for developers to stream data to and from their Apache Kafka clusters. It uses the Kafka Connect framework for connecting Apache Kafka clusters with external systems such as databases, search indexes, and file systems. The guide can be found [here](https://docs.aws.amazon.com/msk/latest/developerguide/msk-connect-getting-started.html).

- **Kafka REST Proxy :**  
The Confluent REST Proxy provides a RESTful interface to an Apache KafkaÂ® cluster, making it easy to produce and consume messages, view the state of the cluster, and perform administrative actions without using the native Kafka protocol or clients. The guide can be found [here](https://docs.confluent.io/platform/current/kafka-rest/index.html).

- **AWS API Gateway :**  
Amazon API Gateway is a fully managed service for creating, publishing, maintaining, monitoring, and securing APIs at any scale. APIs act as the "front door" for applications to access data, business logic, or functionality from your backend services. The documentation can be found [here](https://docs.aws.amazon.com/apigateway/).

- **Apache Spark :**  
Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in multiple languages and an optimized engine that supports general execution graphs.
It also contains higher-level tools including Spark SQL, pandas API on Spark, MLib, GraphX, and Structure Streaming. All documentation can be found [here](https://spark.apache.org/documentation.html).

- **PySpark :**  
PySpark is the Python API for Apache Spark, used to process data with Spark in Python. It supports all of the features mentioned in Apache Spark. Read more [here](https://spark.apache.org/docs/latest/api/python/index.html).

- **AWS MWAA :**  
Amazon Managed Workflows for Apache Airflow (MWAA) is a managed orchestration service for Apache Airflow that sets up and operates data pipelines in the cloud at scale. programmatically author, schedule, and monitor sequences of processes and tasks referred to as workflows. Read more [here](https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html).

- **AWS Kinesis :**  
Amazon Kinesis is used to capture, process, and store video and data streams in real-time for analytics and machine learning. The documentation can be found [here](https://docs.aws.amazon.com/kinesis/).

- **Databricks :**  
The Databricks Lakehouse platform can be used for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. It performs Spark processing of batch and streaming data. The documentation can be found [here](https://docs.databricks.com/en/index.html).

- **AWS EC2 :**  
Amazon Elastic Compute Cloud (Amazon EC2) provides on-demand, scalable computing capacity in the Amazon Web Services (AWS) Cloud. It can launch virtual servers, configure security and networking, and manage storage. The documentation can be found [here](https://docs.aws.amazon.com/ec2/).

- **AWS RDS :**  
Amazon Relational Database Service is a web service that makes it easier to set up, operate, and scale a relational database in the AWS Cloud. The documentation can be found [here](https://docs.aws.amazon.com/rds/).



## 3. Project Walkthrough

### Data Infrastructure Simulating Pinterest

The file user_posting_emulation.py contains the login credentials for a RDS database. Inside the database there are three tables with data resembling data received by the Pinterest API when a POST request is made by a user uploading data to Pinterest
- pinterest_data contains data about posts being updated to Pinterest
- geolocation_data contains data about the geolocation of each Pinterest post found in pinterest_data
- user_data contains data about the user that has uploaded each post found in pinterest_data
 
### 1. Batch processing : Configuring the EC2 Kafka client

To begin, create an MSK cluster. A client is needed to communicate with this MSK cluster - here we will be using an EC2 instance. For this project, both the MSK cluster and an EC2 instance have been provided already, however I will provide the step-by-step to reflect starting from scratch.
 
**To create an MSK cluster:**

Amazon MSK > Create cluster > Choose quick create
> - Name cluster 'pinterest-msk-cluster
> - Choose Cluster type: provisioned
> - Apache Kafka version: 2.8.1
> - Choose Broker type: Kafka.m5.large
> - Amazon EBS storage per broker: 100 GiB
'''
Select create cluster
   
**To create an EC2 instance:**

>- Name: 12a3da8f7ced
>- Instance ID : i-0b0da449b1e54559c
>- Instance type: t2.micro
>- Amazon Machine Image : Amazon linux 2023 AMI
>- Availability Zone: us-east-1a
>- Public IPV4 DNS : ec2-52-206-122-10.compute-1.amazonaws.com
>- Public IPv4 address : 52.206.122.10"
>- Key pair name : 12a3da8f7ced-key-pair
>- Key pair type : RSA
>- Private key file format : .pem
'''
An IAM role was also created for the EC2 to allow Kafka access.


### 1.1 Create .pem file locally
    
Use the key pair assigned at launched of EC2 instance to create .pem file locally, this allows secure local connection to the EC2 instance via SSH.
    
EC2 -> Instances -> Search \"i-0b0da449b1e54559c\" > Details > Key pair assigned at launch.

Copy the key pair and save inside a .pem file, and ensure that the .pem file has read-only permission for User class.

```
chmod 400 12a3da8f7ced-key-pair.pem"
```

   
### 1.2 Connect to the EC2 instance

Follow the instructions on AWS on how to connect to EC2 through the CLI.
```
FOR EXAMPLE: > ssh -i \"12a3da8f7ced-key-pair.pem\"  ec2-user@ec2-52-206-122-10.compute-1.amazonaws.com
```


### 1.3 Installing Kafka on the EC2 client

In order for the client machine to connect and send data to the MSK cluster, it's necessary to edit the inbound rules for the security group associated with the MSK cluster.

VPC -> Security -> Security groups -> Select the default security group associated with the cluster VPC
(The associated Security group can be found in MSK console > properties tab > Networking settings section > Security groups applied)

Edit inbound rules > Add rule:
- Type column: All traffic
- Source column: ID of the security group of the client machine (found in EC2 console > security tab).
- Save rules and cluster will accept all traffic from the client machine.
'''


#### Step 1 : Install Java and Kafka on the EC2 client machine

First, install java and ensure that it is the correct version -> `java -version`.
Next download and 'unzip' kafka version 2.12-2.81. - make sure to install the same version of Kafka as the one the cluster is running on (in this case 2.12-2.8.1) in order for the two to communicate.

```  
sudo yum install java-1.8.0
# Get the version of kafka to install
wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz
# To 'unzip' or 'untar' the file
tar -xzf kafka_2.12-2.8.1.tgz
# Remove the compressed file, keeping only uncompressed version                                         
rm kafka_2.12-2.8.1.tgz 
```
   
#### Step 2 : Installing IAM MSK authentication package on client EC2 machine.

The [IAM MSK authentication package](https://github.com/aws/aws-msk-iam-auth) is required for verifying a client which uses IAM authentication for connecting it to MSK clusters.

Inside the 'kafka_2.12-2.8.1/libs' directory, download the IAM MSK authentication package from Github.

```
cd kafka_2.12-2.8.1/libs
# Download msk iam authentication file
wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.5/aws-msk-iam-auth-1.1.5-all.jar
```

Configure the client classpath environment variable in `.bashrc` so the client is able to use the IAM package.

```
# Open bash configuration file
nano /home/ec2-user/.bashrc
```

```
# .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi

# Uncomment the following line if you don't like systemctl's auto-paging feature:
# export SYSTEMD_PAGER=

# User specific aliases and functions
export CLASSPATH=/home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar
```
 
If everything is configured correctly, `echo $CLASSPATH` returns the class path `/home/ec2-user/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.5-all.jar`.


#### Step 3 : Authenticate MSK cluster using EC2 IAM role

To establish a secure setup, it is essential to configure the EC2 instance with the necessary IAM role and permissions for seamless authentication and interaction with the MSK cluster. Setting up a trust relationship enables the EC2 instance to assume its IAM role. In essence, it declares, "This IAM role for EC2 can be trusted by itself to assume its own role." This IAM role encompasses the permissions required for MSK authentication

To assume the \"12a3da8f7ced-ec2-access-role\" EC2 IAM role, which contains the necessary permissions to authenticate the MSK cluster. First, retrieve the 0a60b9a8a831-ec2-access-role ARN.

IAM console -> Roles -> 12a3da8f7ced-ec2-access-role -> copy ARN  

Trust relationships tab > Edit trust policy > Add a principal
- Selected IAM roles as the Principal type
- - Replace ARN with the 12a3da8f7ced-ec2-access-role ARN (copied from ec2-access-role)


#### Step 4 : Configure Kafka client to use AWS IAM authentication to the cluster

Configure Kafka client to use AWS IAM authentication for authenticating the MSK cluster.
In the EC2 client, inside the `kafka_2.12-2.8.1/bin` directory, usign nano, modify the `client.properties` file as follows:

```
# Sets up TLS for encryption and SASL for authN.
security.protocol = SASL_SSL

# Identifies the SASL mechanism to use.
sasl.mechanism = AWS_MSK_IAM

# Binds SASL client implementation.
sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn="arn:aws:iam::584739742957:role/12a3$
# Encapsulates constructing a SigV4 signature based on extracted credentials.
# The SASL client bound by "sasl.jaas.config" invokes this class.
sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
```

#### 1.4 Creating Kafka topics

The bootstrap server and zookeeper string are required to create Kafka topics on the Kafka cluster using the EC2 client. 

Using the MSK Management Console to get cluster information
Amazon MSK > pinterest-msk-cluster > View client information and note:
- Bootstrap servers Private endpoint (single-VPC)
- Plaintext Apache Zookeeper connection string 

The three topic names being created are:
- 12a3da8f7ced.pin for the Pinterest posts data
- 12a3da8f7ced.geo for the post geolocation data
- 12a3da8f7ced.user for the post user data

In the EC2 client, inside the `kafka_2.12-2.8.1/bin` directory, run the commands:
 
```
./kafka-topics.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 12a3da8f7ced.pin
./kafka-topics.sh --bootstrap-server b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 12a3da8f7ced.geo
./kafka-topics.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --command-config client.properties --create --topic 12a3da8f7ced.user
```


### 2. Batch processing : Connecting the MSK cluster to an S3 bucket.

Next, configure MSK Connect to enable the MSK cluster to automatically transmit and store data to an S3 bucket, that is partitioned by topic. This is achieved by downloading the Confluent.io Amazon S3 Connector and adding it to the S3 bucket through the EC2 client. Then creating a connector in MSK connect by using the newly created custom plugin (which is designed to connect to the S3 bucket)."


##### **Step 1:** Download confluent.io on the EC2 client and copy it to the S3 bucket in the EC2 client.

The s3 bucket our data will be saved in is user-12a3da8f7ced-bucket (IAM role already set up to write to the S3 bucket)
Download the confluent package : 

**Assume admin user privileges**
```
sudo -u ec2-user -i
```

**Create directory where we will save our connector**
```
mkdir kafka-connect-s3 && cd kafka-connect-s3
```

**Download connector from Confluent**
```
wget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-s3/versions/10.0.3/confluentinc-kafka-connect-s3-10.0.3.zip
```

**Copy connector to S3 bucket**
```
aws s3 cp ./confluentinc-kafka-connect-s3-10.0.3.zip s3://user-0a60b9a8a831-bucket/kafka-connect-s3/
```


##### **Step 2 :** Create custom plugin in the MSK Connect console

MSK console > MSK Connect section  > Custom plugins > Create custom plugin.

>- Name this plugin 12a3da8f7ced-plugin\
>- Choose bucket where Confluent connector ZIP file is located (s3://user-12a3da8f7ced-bucket/kafka-connect-s3/confluentinc-kafka-connect-s3-10.0.3.zip)


##### **Step 3:** Create a connector in MSK connect using custom plugin to connect to S3.

'''
MSK connect > Customised plugins > choose 12a3da8f7ced-plugin > Create connector > Connector properties
> - **Basic properties**
> - Connector name : 12a3da8f7ced-connector
> - Description â optional : Connecting topics to s3 bucket
> - **Apache Kafka cluster**
> - Cluster type : MSK cluster
> - MSK clusters : pinterest-msk-cluster
> - **Connector configuration**
> - Configuration settings :  

'''
connector.class=io.confluent.connect.s3.S3SinkConnector  
s3.region=us-east-1    
flush.size=1   
schema.compatibility=NONE  
tasks.max=3     
topics.regex=12a3da8f7ced.*     
format.class=io.confluent.connect.s3.format.json.JsonFormat     
partitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner   
value.converter.schemas.enable=false    
value.converter=org.apache.kafka.connect.json.JsonConverter     
storage.class=io.confluent.connect.s3.storage.S3Storage   
key.converter=org.apache.kafka.connect.storage.StringConverter 
s3.bucket.name=user-12a3da8f7ced-bucket"
'''
 
**Connector capacity**
>    - Capacity type : Provisioned
>    - MCU count per worker : 1
>    - Number of workers : 1
> **Worker configuration**
>    -Use a customised configuration
>    - Worker configuration : confluent worker
> **Access permision**
>    - IAM role : 12a3da8f7ced-ec2-access-role
  

### 3. Batch Processing : Configuring an API in API Gateway

#### 3.1 Build a Kafka REST proxy integration method for the API
 
An API is needed to send data to the MSK cluster and in turn, the S3 bucket.
  
##### **Step 1:** Create a REST API: 
 
API gateway > Create API > REST API > Build > 
>- API name: 12a3da8f7ced
>- >- Remaining settings remain as default. 
 
##### **Step 2:** Create a resource that allows building a PROXY integration for the API and setup a HTTP ANY method onto the created resource.
    
>- Integration type: HTTP Proxy
>- HTTP method: ANY
>- Endpoint URL: http://ec2-52-206-122-10.compute-1.amazonaws.com:8082/{proxy} (EC2 instance as endpoint)
>- Content handling: Passthrough (To pass all payloads to backend, apply a mapping template if specified)


##### **Step 3:** Deploy the API and note the invoke URL so it can be used for POST requests.   
Invoke URL : https://amdpzoul4j.execute-api.us-east-1.amazonaws.com/test


### 3.2 Set up the Kafka REST proxy on EC2 client machine
 
Now that the API has been set up to send data to the EC2 client. Install the Confluent package to setup a REST proxy API on EC2 client which listens for requests and interacts with the kafka cluster.


##### **Step 1:** Install Confluent package for Kafka REST proxy on EC2 client machine. 

```
sudo wget https://packages.confluent.io/archive/7.2/confluent-7.2.0.tar.gz      
tar -xvzf confluent-7.2.0.tar.gz 
```

##### **Step 2:** Allow the REST proxy to perform IAM authentication to the MSK cluster by modifying the kafka-rest.properties file
Navigate to `confluent-7.2.0/etc/kafka-rest`, and modify the `kafka-rest.properties` file.

Inside the `kafka-rest.properties` file. Modify the bootstrap.servers and the zookeeper.connect variables in this file, with the corresponding Boostrap server string and Plaintext Apache Zookeeper connection string, gathered back in section 1.4.

To surpass the IAM authentication of the MSK cluster, we will make use of the IAM MSK authentication package again, adding this at the bottom of `kafka-rest.properties`.

```
# Copyright 2018 Confluent Inc.
#
# Licensed under the Confluent Community License (the "License"); you may not use
# this file except in compliance with the License.  You may obtain a copy of the
# License at
#
# http://www.confluent.io/confluent-community-license
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OF ANY KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations under the License.
#
#id=kafka-rest-test-server
#schema.registry.url=http://localhost:8081
zookeeper.connect=z-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:2181,z-1.pinterestmskcluster.w8g8jt.$bootstrap.servers=b-2.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098,b-1.pinterestmskcluster.w8g8jt.$#
# Configure interceptor classes for sending consumer and producer metrics to Confluent Control Center
# Make sure that monitoring-interceptors-<version>.jar is on the Java class path
#consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
#producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor

# Sets up TLS for encryption and SASL for authN.
client.security.protocol = SASL_SSL

# Identifies the SASL mechanism to use.
client.sasl.mechanism = AWS_MSK_IAM

# Binds SASL client implementation.
client.sasl.jaas.config = software.amazon.msk.auth.iam.IAMLoginModule required awsRoleArn="arn:aws:iam::584739742957:ro$

# Encapsulates constructing a SigV4 signature based on extracted credentials.
# The SASL client bound by "sasl.jaas.config" invokes this class.
client.sasl.client.callback.handler.class = software.amazon.msk.auth.iam.IAMClientCallbackHandler
```


##### **Step 3:** Start the REST proxy on the EC2 client machine
To make sure messages are consumed in MSK, start the REST proxy in 'confluent-7.2.0/bin' (this also functions as a test).   
Showing the INFO Server started and listening for requests inside EC2 console."

```
./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties"
```

#### 3.3 Sending data to the API

Modify the `user_posting_emulation.py` file to send data to Kafka topics using the API Invoke URL. Send data from the three tables to their corresponding Kafka topic.

Open three separate terminals, each inside the `kafka_2.12-2.8.1/bin` directory of the ec2 client, Then set up the kafka consumers, as shown in the code below (one terminal per topic, for the three topics).

Running `user_posting_emulation.py` on the local command line posts data messages to the cluster via the API gateway and the kafka REST proxy.
    
All three consumer terminals alongside the REST proxy terminal actively streamed data, with the correct data consumed by the corresponding topic. Messages also showed up in the S3 bucket, inside a folder named 'Topics'.

```
    "./kafka-console-consumer.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 12a3da8f7ced.pin --from-beginning\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-1.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 12a3da8f7ced.geo --from-beginning\n",
    "\n",
    "./kafka-console-consumer.sh --bootstrap-server b-3.pinterestmskcluster.w8g8jt.c12.kafka.us-east-1.amazonaws.com:9098 --consumer.config client.properties --group students --topic 12a3da8f7ced.user --from-beginning"
```
 
### 4. Batch processing : Databricks

**Mount AWS S3 bucket to Databricks**  
In order to start cleaning and querying batch data, mount and read the data from the s3 bucket into Databricks. 

To begin, a notebook - `pinterest_authenticate_aws` - was made to create a function which reads and extracts a delta table containg the AWS authentication keys:
1. Read the credentials delta table into a Sparks dataframe
2. Create variables using AWS access key and secret key from the spark dataframe and return these variables in the function

Another notebook - `mount_S3_bucket` - was created to contain the function to mount the S3 bucket to Databricks:
1. Inisde of the notebook, run the 'pinterest_authenticate_aws' notebook to retrieve the AWS access key and secret key
2. Mount the S3 bucket using the AWS key credentials
This Notebook only needs to be run once to mount the S3 bucket.
    
To manage the batch data, a third notebook - `pinterest_batch_data_and_cleaning` - was created to read the data from the s3 bucket and convert its contents into a dataframe. The same notebook would later be used to run all cleaning functions ready for querying:
1. Run the `pinterest_authenticate_aws` notebook to retrieve the AWS access key and secret key 
2. Read all data with a .json file extension from the S3 bucket
3. Convert the data into a spark dataframe and dynamically generate the dataframe name
4. Create three different DataFrames:
   - df_pin for the Pinterest post data
   - df_geo for the geolocation data     
   - df_user for the user data"


### 5. Clean all three dataframes and query the data on databricks using pyspark
Clean the three dataframes df_pin, df_geo, df_user and query the data on databricks using pyspark.

The data was then queried in a separate notebook - `query-pinterest-batch-data` - using pyspark for the following questions:

1. Find the most popular Pinterest category people post to based on their country.
2. Find the most popular category in each year between 2018 and 2022
3. Find the user with most followers in each country
4. What is the most popular category people post to for different age groups?
5. What is the median follower count for users in different age groups?
6. Find how many users have joined each year between 2015 and 2020
7. Find the median follower count of users based on their joining year between 2015 and 2020.
8. Find the median follower count of users based on their joining year and age group for 2015 to 2020.


### 6. Batch processing: AWS MWAA

**Orchestrating databricks workload using AWS MWAA**
Upload the `12a3da8f7ced_dag.py` directed acyclic graph (DAG) file to the S3 bucket `mwaa-dags-bucket/dags` associated with the MWAA environment. This allows us to run the DAG from the AWS airflow UI.
Utilise AWS Managed Workflows for Apache Airflow (MWAA) to automate `@daily` batch processing of the previously created databricks notebook.
  

#### 6.1 Create and upload DAG to an MWAA environment 

MWAA requires connection to an S3 bucket in order to hold Directed Acyclic Graphs (DAGs), Python requirements, and plugins. Following this the MWAA airflow console can then be used to run the DAGs.  
    
This project already provided me with an MWAA environment linked to an S3 bucket. However in instances where this isn't the case, this can be done through the following steps: 

##### **Step 1:** Create an S3 bucket

Amazon S3 > Bucket > create bucket:     
>- General configuration: 
>    - AWS region : US East (N. Virginia) us-east-1  
>    - Bucket type : General purpose
>    - Bucket name: mwaa-dags-bucket
>- Block Public Access settings for this bucket: 
>    - Block all public access 
>- Bucket Versioning: 
>    - Enable   
'''
    
After creating the bucket, create a folder named `dags` inside it.
   
##### **Step 2:** Creating the MWAA environment

Amazon MWAA (us-east-1) > create environment: 
>- Environment details:  
>    - Name: Databricks-Airflow-env 
>- DAG code in Amazon S3:
>    - S3 Bucket : s3://mwaa-dags-bucket  (browse S3 and choose the previously created bucket)  
>    - DAGs folder : s3://mwaa-dags-bucket/dags (choose the dags folder created inside the S3 bucket)   
>- Then on networking page:   
>    - select create MWAA VPC   
>    - Choose the preferred Apache Airflow access mode
>    - Web server access : Private network  
>    - Security groups : Create new security group  
>- Environment class:    
>    - Class : mw1.small (recommended to choose the smallest environment size that is necessary to support the workload) 
>    - Maximum worker count : 5  
>    - Minimum worker count : 1  
>    - Scheduler count : 2   
'''


##### **Step 3:** Create an API token in Databricks

The API token functions as a connection between Databricks and the MWAA environment.
Username > User Settings > Access tokens > Generate new token > Copy the Token ID.  


##### **Step 4:** Connect MWAA to Databricks

Using the Databricks API token, set up the connection between MWAA and Databricks

Amazon MWAA > Environments > Databricks-Airflow-env > Open Airflow UI > Admin > Connections > databricks_default > Edit record:
>- Host column : \\<url of your Databricks account\\>
>- Extra column : {\"token\": \"\\<API_token_id_from_previous_step\\>\", \"host\": \"\\<url_from_host_column\\>\"}
>- Connection type column: Databricks
'''

##### **Step 5:** Obtaining Databricks connection type

Obtaining Databricks connection type requires installalation of the corresponding Python dependencies for the MWAA environment This is included in the `requirements.txt` file. 
Before uploading a `requirements.txt` file in the `mwaa-dags-bucket` S3 bucket, the following [Github repository](https://github.com/aws/aws-mwaa-local-runner) can be used to create and test the environment.
    
After uploading requirements.txt to the S3 bucket, select the `requirements.txt` environment in amazon MWAA.
Amazon MWAA > environment > select environment > edit:  
>- DAG code in Amazon S3:  
>    - Requirements file : s3://mwaa-dags-bucket/requirements.txt 
   

#### 6.2 Upload DAG to MWAA environment and trigger the DAG
  
Create an Airflow DAG that will trigger a Databricks Notebook by upload the corresponding `0a60b9a8a831_dag` Python file in the `mwaa-dags-bucket/dags` S3 bucket folder (associated with the MWAA environment).

To manually trigger the DAG:
Amazon MWAA > Environments > Databricks-Airflow-env > Airflow UI from the MWAA environment.    
Unpause the DAG from AWS MWAA airflow UI, and trigger the DAG.

### 7. Stream processing: AWS Kinesis  

To stream data from the AWS RDS to Databricks Delta tables, a new script was created to send data to an API and creating a new notebook to stream to Delta tables.

#### 7.1 Creating streams on AWS Kinesis
First, the following 3 streams were created to link the three Pinterest tables:  

- streaming-12a3da8f7ced-pin 
- streaming-12a3da8f7ced-geo    
- streaming-12a3da8f7ced-user  
'''

On AWS Kinesis > Create new stream
  
#### 7.2 Modifying the REST API
    
Configure the previously created REST API to allow it to invoke Kinesis actions.
AWS account has been granted permissions to invoke Kinesis actions, there was no need to create an IAM role for API to access Kinesis. However this can be done with the following steps:    
    
Copy the ARN of the access role `12a3da8f7ced-kinesis-access-role` from the IAM console, under Roles. ARN to be used when setting up the Execution role for the integration point of all the API methods created.
  

##### **Step 1: List streams in Kinesis**

To begin building the integration, navigate to the previously created API in AWS API Gateway. And select Create resource
API Gateway > APIs > previously created API > Create resource:
    
    
Under this newly created streams resource, create a **GET** method with the following settings: 

>- Integration type: AWS Service    
>- AWS Region: us-east-1    
>- AWS Service: Kinesis  
>- HTTP method: POST (to invoke Kinesis's ListStreams action)   
>- Action Type: Use action name    
>- Action name: ListStreams 
>- Execution role: ARN of your Kinesis Access Role (created in the previous section)
'''
 
Once the GET method has been created, select the Integration request panel, and click on the Edit and modify the header parameteres and mapping template.

URL Requests header parameters: 
>- Name: Content-Type   
>- Mapped from: 'application/x-amz-json-1.1'
    
Mapping templates: 
>- Content type: application/json
>-Body:  {}


##### **Step 2: Create, describe and delete streams in Kinesis**  
 
Under the streams resource create a new child resource with the Resource name {stream-name}

**The settings to create a POST method:**  
On the create method settings:

>- Integration Type: AWS Service   
>- AWS Region: us-east-1    
>- AWS Service: Kinesis     
>- HTTP method: POST    
>- Action: CreateStream     
>- Execution role: ARN of IAM role created
'''

In 'Integration Request' under 'Mapping Templates', add new mapping template:
>- Content Type: 'application/json'
>- Mapping Template Body:"
```
{
    "ShardCount": #if($input.path('$.ShardCount') == '') 5 #else $input.path('$.ShardCount') #end,
    "StreamName": "$input.params('stream-name')"
}
```
 
**For the other methods, the same settings were used except for the action name while creating the method and mapping template for the integration request settings:**
   
**The settings to create the GET method:** 
>- Action name: 'DescribeStream'    
>- Mapping Template:
```
{
    "StreamName": "$input.params('stream-name')"
}
```

**The settings to create the DELETE method:**

>- Action: 'DeleteStream'
>- Mapping Template Body:

```
{
    "StreamName": "$input.params('stream-name')"
}
```


##### **Step 3: Add records to streams in Kinesis** 

Under {stream-name} resource create a two new child resources with the Resource Name: record and records. THe settings for creating the child resources were as follows:

>- Resource path : /streams/stream-name/
>- Resource name : record
   
>- Resource path : /streams/stream-name/ 
>- Resource name : records 

  
For both resources create a PUT method.

**The settings to create the PUT method for record:** 
>- Action name: 'PutRecord'  
>- Mapping Template:   
```
{
    "StreamName": "$input.params('stream-name')",
    "Data": "$util.base64Encode($input.json('$.Data'))",
    "PartitionKey": "$input.path('$.PartitionKey')"
} 
```
 
**The settings to create the PUT method for records:**  
>- Action name: 'PutRecords'
>- Mapping Template: 
```
{
    "StreamName": "$input.params('stream-name')",
    "Records": [
       #foreach($elem in $input.path('$.records'))
          {
            "Data": "$util.base64Encode($elem.data)",
            "PartitionKey": "$elem.partition-key"
          }#if($foreach.hasNext),#end
        #end
    ]
}
```

After this, re-deploy the API and make note of the invoke URL once again


### 7.3 Sending Data to the Kinesis streams    
  
For sending data to kinesis streams the 'user_posting_emulation.py' script was modified to create an apiThe previously created script `user_posting_emulation.py` was modified to create an api_send_to_kinesis function. Create a new script `user_posting_emulation_streaming.py` to send requests to the newly created API, sending data one at a time from the three Pinterest AWS RDS tables to their corresponding Kinesis streams.


### 7.4 Read data from Kinesis streams into Databricks   
    
Create a new notebook called `pinterest_streaming_data_and_cleaning` in Databricks.

1. To read the credentials import the `pinterest_authenticate_aws` notebook.
2. Create functions to ingest data into Kinesis Data Streams.
3. Read the data from the three streams.


### 7.5 Transform Kinesis Streams in Databricks

Clean the streaming data using the same methods adopted for batch cleaning. Display the data to check transformations.
 

### 7.6 Write streaming data into Databricks delta tables

Saved each stream to a table through write_stream_df_to_table(<name>)

The following three tables were created:

- 12a3da8f7ced_pin_table
- 12a3da8f7ced_geo_table
- 12a3da8f7ced_user_table


## Looking Forward 
- After Databricks conversions in the Pinterest data pipeline, integrate Power BI or Tableau for visual insights into data flow, processing stages, and key metrics. Enhance monitoring, troubleshoot efficiently, and validate data quality, contributing to a comprehensive understanding of pipeline performance.
- Focus on enhancing production readiness by refining the code for robustness and efficiency. Specifically, tighten IAM policies to restrict access, implement the principle of least privilege, and ensure overall security. This guarantees a secure and compliant deployment, crucial for sustained operation and integrity of the data processing pipeline.
